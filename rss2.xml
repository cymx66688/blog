<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Kelley&#39;s blog</title>
    <link>https://cymx66688.github.io/blog/</link>
    
    <atom:link href="/blog/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>不定时更新机器学习算法原理及实践</description>
    <pubDate>Thu, 18 Jun 2020 01:15:47 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>Hello World</title>
      <link>https://cymx66688.github.io/blog/archives/hello-world/</link>
      <guid>https://cymx66688.github.io/blog/archives/hello-world/</guid>
      <pubDate>Thu, 18 Jun 2020 01:15:47 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
        
      
      </description>
      
      
      <content:encoded><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content:encoded>
      
      <comments>https://cymx66688.github.io/blog/archives/hello-world/#disqus_thread</comments>
    </item>
    
    <item>
      <title>深度学习概述</title>
      <link>https://cymx66688.github.io/blog/archives/DLOverview/</link>
      <guid>https://cymx66688.github.io/blog/archives/DLOverview/</guid>
      <pubDate>Mon, 01 Jun 2020 01:40:15 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;该文作为深度学习篇的开篇介绍，对深度学习作一些简单概述，后续再具体介绍CNN、RNN、LSTM、GAN每个算法。在学习深度学习前，希望有机器学习的相关知识，可以更加容易理解。&lt;/p&gt;
&lt;h2 id=&quot;人工智能、机器学习、深度学习的关系&quot;&gt;&lt;a href=&quot;#人工智能、机器学
        
      
      </description>
      
      
      <content:encoded><![CDATA[<p>该文作为深度学习篇的开篇介绍，对深度学习作一些简单概述，后续再具体介绍CNN、RNN、LSTM、GAN每个算法。在学习深度学习前，希望有机器学习的相关知识，可以更加容易理解。</p><h2 id="人工智能、机器学习、深度学习的关系"><a href="#人工智能、机器学习、深度学习的关系" class="headerlink" title="人工智能、机器学习、深度学习的关系"></a>人工智能、机器学习、深度学习的关系</h2><p><img src="/archives/DLOverview/AI、ML、DL关系图.jpg" alt="AI、ML、DL关系图" style="zoom: 33%;"></p><p>上图通俗易懂地讲明了三者之间的关系。人工智能是广义的概念，它包含机器学习和深度学习，而机器学习包含深度学习。另一种方式讲述就是机器学习包括传统机器学习(如Logistics、DecisionTree等)和深度学习(如CNN、RNN等)。</p><p>机器视觉(CV)和自然语言处理(NLP)是交叉的算法领域，深度学习、机器学习的知识都会运用到，因此，学习机器学习和深度学习的算法，了解底层原理在算法工作中是必不可少的一个环节。</p><h2 id="深度学习的发展史"><a href="#深度学习的发展史" class="headerlink" title="深度学习的发展史"></a>深度学习的发展史</h2><ul><li><p>起源阶段：</p><ul><li>1943年，心理学家麦卡洛克和数学逻辑学家皮兹发表论文《神经活动中内在思想的逻辑演算》，提出了MP模型。</li><li>1949年，加拿大著名心理学家唐纳德·赫布在《行为的组织》中提出了一种基于无监督学习的规则——海布学习规则(Hebb Rule)。</li><li>20世纪50年代末，在MP模型和海布学习规则的研究基础上，美国科学家罗森布拉特发现了一种类似于人类学习过程的学习算法——感知机学习。吸引了大量科学家对人工神经网络研究的兴趣，对神经网络的发展具有里程碑式的意义，但是存在<strong>异或问题</strong>。</li></ul></li><li><p>发展阶段：</p><ul><li>1986年，深度学习之父杰弗里·辛顿提出了一种适用于多层感知器的反向传播算法——BP算法。<strong>BP算法完美的解决了非线性分类问题。</strong></li></ul></li><li>爆发阶段：<ul><li>2006年，杰弗里·辛顿以及他的学生鲁斯兰·萨拉赫丁诺夫正式提出了深度学习的概念。他们在世界顶级学术期刊《科学》发表的一篇文章中详细的给出了“梯度消失”问题的解决方案——<strong>通过无监督的学习方法逐层训练算法，再使用有监督的反向传播算法进行调优</strong>。</li><li>2012年，在著名的ImageNet图像识别大赛中，杰弗里·辛顿领导的小组采用深度学习模型AlexNet一举夺冠。</li><li>2016年，随着谷歌公司基于深度学习开发的AlphaGo以4:1的比分战胜了国际顶尖围棋高手李世石。</li></ul></li></ul><h2 id="神经网络的起源"><a href="#神经网络的起源" class="headerlink" title="神经网络的起源"></a>神经网络的起源</h2><p>神经网络分为浅层神经网络和深层神经网络，深层神经网络就是现在的深度学习。</p><p><img src="/archives/DLOverview/生物神经元.png" alt="生物神经元" style="zoom:50%;"></p><p>神经网络的灵感来自于生物神经元。其有四个特性：</p><ol><li>每个神经元都是一个多输入单输出的信息处理单元； </li><li>神经元具有空间整合和时间整合特性；  </li><li><p>神经元输入分兴奋性输入和抑制性输入两种类型；  </p></li><li><p>神经元具有阈值特性。</p></li></ol><p>通过生物神经元的四个特性对应到神经网络中:</p><ul><li><p>多输入单输出&amp;空间整合 ——>多输入信号累加<script type="math/tex">\Sigma_{i=0}^{i}x_{i}</script></p></li><li><p>兴奋性/抑制性输入——&gt;权重$w_i$的正负模拟兴奋和抑制。</p></li><li>阈值特性——&gt;输入超过阈值$\theta$,则被激活，信号输入到下一层神经元。</li></ul><p><strong>总结：</strong></p><p>我们可以大体得到一个神经网络的模型，就是权重$w_i$*输入值$x_i$的累加和形成线性函数，外面套一层激活函数，将线性转换成非线性。</p><p>函数表示为：</p><script type="math/tex; mode=display">\theta(\Sigma^i_{i=1}w_ix_i+b)</script><p>那么接下来我们说说激活函数。</p><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><h3 id="增加激活函数的意义"><a href="#增加激活函数的意义" class="headerlink" title="增加激活函数的意义"></a>增加激活函数的意义</h3><p>如果没有激活函数，相当于是矩阵相乘，只能拟合线性函数，而在大多数情况下，线性关系的模型只能简单处理输入与输出之间的关系，而增加激活函数，非线性变换能够学习到两者间更复杂的变换关系。</p><h3 id="常见的激活函数"><a href="#常见的激活函数" class="headerlink" title="常见的激活函数"></a>常见的激活函数</h3><ul><li><strong>Sigmoid激活函数</strong></li></ul><script type="math/tex; mode=display">\color{blue}\sigma(z)=\frac{1}{1+e^-z}</script><script type="math/tex; mode=display">\sigma(z)^\prime=\sigma(z)(1-\sigma(z))</script><p>优点：可将一个实数映射到(0, 1)之间，用作二分类。</p><p>缺点：在z&lt;-5或z&gt;5时，导数趋于0，会导致梯度消失。</p><p><img src="/archives/DLOverview/sigmoid.jpg" alt="sigmoid" style="zoom:50%;"></p><ul><li><strong>tanh激活函数</strong></li></ul><script type="math/tex; mode=display">\color{blue}\tanh(x)=2\sigma(2x)-1=\frac{e^z-e^{-z}}{e^z+e^{-z}}</script><script type="math/tex; mode=display">\tanh(x)^\prime=1-\tanh(x)^2</script><p>优点：取值范围为[-1, 1]，在特征相差明显时效果会更好，在循环过程中会不断扩大特征效果。</p><p>缺点：类似sigmoid函数，容易导致梯度消失。</p><p><img src="/archives/DLOverview/tanh.jpg" alt="tanh" style="zoom:70%;"></p><ul><li><strong>ReLU激活函数</strong></li></ul><script type="math/tex; mode=display">\color{blue}relu(z)=max(0,z)</script><p>当$z&gt;0$时，$relu(z)\prime=1$; 当$z\leq=0$时，$relu(z)\prime=0$</p><p>优点：函数简单，反向求导时计算量少。</p><p>缺点：当$z\leq=0$时，会出现梯度消失问题。</p><p><img src="/archives/DLOverview/relu.png" alt="relu" style="zoom:50%;"></p><ul><li><strong>Leaky ReLU激活函数</strong></li></ul><p>$\color{blue}leakyrelu(z)=max(0.01z, z)$</p><p>优点：在ReLU的基础上，在$z\leq=0$时给予一个很小的权重，求导后结果很小，但是一个定值，不会出现梯度消失的问题，又结合了ReLU本身的优点。</p><p><img src="/archives/DLOverview/leakyrelu.jpg" alt="leakyrelu" style="zoom:50%;"></p><h2 id="神经网络单层公式拆解"><a href="#神经网络单层公式拆解" class="headerlink" title="神经网络单层公式拆解"></a>神经网络单层公式拆解</h2><p>数学公式: $\vec{y}=\sigma(W\cdot\vec{x}+b)$</p><p>$W\cdot\vec{x}$  升维/降维/放大/缩小/旋转</p><p>$+b$  平移</p><p>$\sigma$  弯曲/非线性</p><h3 id="更宽-or-更深？"><a href="#更宽-or-更深？" class="headerlink" title="更宽 or 更深？"></a>更宽 or 更深？</h3><p><img src="/archives/DLOverview/更宽or更深.png" alt="更宽or更深"></p><p><img src="/archives/DLOverview/更宽or更深2.png" alt="更宽or更深2"></p><p>从上图结果中可以看到，在神经元总数相当的情况下，增加网络深度比增加网络宽度带来更强的网络表示能力，错误率也降低明显。</p><p>深度和宽度对函数复杂度的贡献是不同的，深度的贡献呈指数级增长，宽度的贡献呈线性增长。</p><p>$FC=\prod^d_{l=1}(\alpha\cdot\theta_l)^{\beta_l}$</p><p>其中$\alpha$表示每层参数(宽度)对函数复杂度的贡献，$\theta$表示参数数量，$\beta$表示深度对函数复杂度的贡献，$\alpha$和$\beta$都是一个区间，即相同参数在不同数值下仍然有不同的复杂度。$d$表示深度，$l$表示第$l$层。</p><h2 id="深度神经网络-DNN"><a href="#深度神经网络-DNN" class="headerlink" title="深度神经网络(DNN)"></a>深度神经网络(DNN)</h2><h3 id="DNN介绍"><a href="#DNN介绍" class="headerlink" title="DNN介绍"></a>DNN介绍</h3><p>DNN(Deep Neural Networks)是前馈神经网络，训练方法是BP算法。</p><p>DNN内部的神经网络层分为三类，输入层、隐藏层、输出层。层与层之间是全连接，从局部来看还是一个线性关系$z=\Sigma_{i=1}^i{w_ix_i}+b$加上一个激活函数$\sigma(z)$。</p><p><img src="/archives/DLOverview/神经网络.png" alt="神经网络"></p><h3 id="DNN前向传播"><a href="#DNN前向传播" class="headerlink" title="DNN前向传播"></a>DNN前向传播</h3><p>前向传播算法就是通过输入层$x_i$的输入计算隐藏层的输出，再将隐藏层的结果作为输入计算输出层的结果，即利用上一层的输出计算下一层的输出。</p><p><img src="/archives/DLOverview/前向传播.png" alt="前向传播" style="zoom:40%;"></p><p>以上图为例：$LayerL_1$是输入层，$LayerL_2$是隐藏层，$LayerL_3$是输出层。</p><p>计算$LayerL_2$的输出$a_1^{(2)}$，$a_2^{(2)}$，$a_3^{(2)}$：</p><script type="math/tex; mode=display">a_1^{(2)}=\sigma(z_1^{(2)})=\sigma(\sum_{k=1}^3{w_{1k}^{(2)}x_k}+b_1^{(2)})</script><script type="math/tex; mode=display">a_2^{(2)}=\sigma(z_2^{(2)})=\sigma(\sum_{k=1}^3{w_{2k}^{(2)}x_k}+b_2^{(2)})</script><script type="math/tex; mode=display">a_3^{(2)}=\sigma(z_3^{(2)})=\sigma(\sum_{k=1}^3{w_{3k}^{(2)}x_k}+b_3^{(2)})</script><p>计算$LayerL_3$的输出$a_1^{(3)}$:</p><script type="math/tex; mode=display">a_1^{(3)}=\sigma(z_1^{(3)})=\sigma(\sum_{k=1}^3{w_{1k}^{(3)}a_k^{(2)}}+b_1^{(3)})</script><p>将公式一般化，假设第$l-1$层有$m$个神经元，对于第$l$层的第$j$个神经元的输出为$a_j^l$，公式为：</p><script type="math/tex; mode=display">a_j^l=\sigma(z_j^l)=\sigma(\sum_{k=1}^mw_{jk}^la_k^{l-1}+b_j^l)</script><p>其中：当$l$=2时，$a_k^1$即为输入层的$x_k$</p><p>将代数公式改为向量公式，第$l$层的输出为：</p><script type="math/tex; mode=display">a^l=\sigma(z^l)=\sigma(W^la^{l-1}+b^l)</script><h3 id="DNN反向传播"><a href="#DNN反向传播" class="headerlink" title="DNN反向传播"></a>DNN反向传播</h3><p>BP算法(Back Propagation)是由学习过程由信号的<strong>正向传播</strong>与<strong>误差的反向传播</strong>两个过程组成。由于多层前馈网络的训练经常采用误差反向传播算法，也常将多层前馈网络直接称为BP网络。</p><p>反向传播，顾名思义就是根据已知的输出结果反向更新权重$W$。</p><p>先拆解复合函数</p><p><img src="/archives/DLOverview/复合函数.png" alt="复合函数"></p><p>对复合函数链式求导</p><script type="math/tex; mode=display">\frac{\partial{L}}{\partial{w_i}}=\frac{\partial{L}}{\partial{a}}\cdot\frac{\partial{a}}{\partial{z}}\cdot\frac{\partial{z}}{\partial{w_i}}</script><p>假设$L=\sum_{k=1}^c(a_k-y_k)^2$   $a(z)=\sigma(z)=\frac{1}{1+e^{-z}}$</p><p>则求导为：</p><script type="math/tex; mode=display">\frac{\partial{L}}{\partial{w_i}}=2(a-y)\cdot{a}(1-a)\cdot{x_i}</script><p>Sigmoid函数的特性是当z&gt;5时，$a(z)$趋于1，当z&lt;-5时，$a(z)$趋于0。从求导公式中可以看到当a=0或a=1时，导数为0，这也验证了sigmoid激活函数会出现梯度消失的原因。</p><h2 id="深度学习主流开发框架"><a href="#深度学习主流开发框架" class="headerlink" title="深度学习主流开发框架"></a>深度学习主流开发框架</h2><ul><li>TensorFlow2.0(1.X版本有比较多的坑，建议学习2.0版本的TensorFlow)</li><li>PyTorch</li><li>Keras</li></ul><p>建议：可以先学习其中的一种，学会后再扩展其他框架，原理都是类似的。</p><h2 id="深度学习局限性"><a href="#深度学习局限性" class="headerlink" title="深度学习局限性"></a>深度学习局限性</h2><ul><li>算法输出不稳定，容易被”攻击”(图像领域加噪声后容易分类错误;问答领域随机加入词，准确率明显下降)。</li><li><p>模型复杂度高，难以纠错和调试(alphaGo)。</p></li><li><p>模型层级复合程度高，参数不透明。</p></li><li><p>端到端训练方式对数据依赖性强，模型增量性差(深度学习只有数据量大时，才能体验它的优势)。</p></li><li><p>专注直观感知类问题，对开放性推理问题无能为力。</p></li><li><p>人类知识无法有效引入进行监督，机器偏见难以避免(种族歧视问题: 算法依赖于大数据，但数据不是中立的：从真实社会中抽取，必然带有社会固有的不平等、排斥性和歧视)。</p></li></ul><p><strong>总结：深度学习仍存在稳定性低、可调式性差、参数不透明、机器偏见、增量性差、推理能力差的问题。</strong></p><p>参考文献：</p><ol><li>深度学习发展史 <a href="https://zhuanlan.zhihu.com/p/34472753" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/34472753</a></li><li>深度神经网络（DNN）模型与前向传播算法 <a href="https://www.cnblogs.com/pinard/p/6418668.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6418668.html</a></li></ol>]]></content:encoded>
      
      <comments>https://cymx66688.github.io/blog/archives/DLOverview/#disqus_thread</comments>
    </item>
    
    <item>
      <title>算法工程师软件工具推荐</title>
      <link>https://cymx66688.github.io/blog/archives/%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88%E8%BD%AF%E4%BB%B6%E5%B7%A5%E5%85%B7%E6%8E%A8%E8%8D%90/</link>
      <guid>https://cymx66688.github.io/blog/archives/%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88%E8%BD%AF%E4%BB%B6%E5%B7%A5%E5%85%B7%E6%8E%A8%E8%8D%90/</guid>
      <pubDate>Mon, 18 May 2020 02:37:25 GMT</pubDate>
      <description>
      
        常用工具推荐
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="基本工具"><a href="#基本工具" class="headerlink" title="基本工具"></a>基本工具</h1><p>① PyCharm  <a href="https://www.jetbrains.com/pycharm/" target="_blank" rel="noopener">https://www.jetbrains.com/pycharm/</a></p><p>② Jupyter  <a href="https://www.anaconda.com/products/individual" target="_blank" rel="noopener">https://www.anaconda.com/products/individual</a></p><p>③Navicat  <a href="https://pan.baidu.com/s/1xsqC19uUSJtMm7Cq6NgMZQ" target="_blank" rel="noopener">https://pan.baidu.com/s/1xsqC19uUSJtMm7Cq6NgMZQ</a>  提取码：kzfp</p><p>④Xshell  <a href="https://pan.baidu.com/s/11yrXYoz2nn8mtE84h6zfEQ" target="_blank" rel="noopener">https://pan.baidu.com/s/11yrXYoz2nn8mtE84h6zfEQ</a>  提取码：zt58</p><p>⑤Notepad++  <a href="https://pan.baidu.com/s/1_Lfe8zNFV2VGHSga0SiwFg" target="_blank" rel="noopener">https://pan.baidu.com/s/1_Lfe8zNFV2VGHSga0SiwFg</a>  提取码：lfli</p><h1 id="辅助工具"><a href="#辅助工具" class="headerlink" title="辅助工具"></a>辅助工具</h1><h2 id="思维导图"><a href="#思维导图" class="headerlink" title="思维导图"></a>思维导图</h2><p>①幕布  <a href="http://mubu.com/" target="_blank" rel="noopener">http://mubu.com/</a></p><p>②XMind ZEN  <a href="https://pan.baidu.com/s/1f4d2XHum4l3ke30JgyDXxg" target="_blank" rel="noopener">https://pan.baidu.com/s/1f4d2XHum4l3ke30JgyDXxg</a>  提取码：zlxj</p><p>③MindMaster  <a href="https://pan.baidu.com/s/1EcYqgFmoXY3r1Uj8y6Wvqw" target="_blank" rel="noopener">https://pan.baidu.com/s/1EcYqgFmoXY3r1Uj8y6Wvqw</a>  提取码：dgs9</p><h2 id="Markdown"><a href="#Markdown" class="headerlink" title="Markdown"></a>Markdown</h2><p>Typora  <a href="https://www.typora.io/" target="_blank" rel="noopener">https://www.typora.io/</a></p><p>优点：What You See Is What You Mean</p><h2 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h2><p>TeamViewer  <a href="https://pan.baidu.com/s/1ouH-uQLSCkZpyjmCWVhudQ" target="_blank" rel="noopener">https://pan.baidu.com/s/1ouH-uQLSCkZpyjmCWVhudQ</a>  提取码：70xn</p>]]></content:encoded>
      
      <comments>https://cymx66688.github.io/blog/archives/%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88%E8%BD%AF%E4%BB%B6%E5%B7%A5%E5%85%B7%E6%8E%A8%E8%8D%90/#disqus_thread</comments>
    </item>
    
    <item>
      <title>test555</title>
      <link>https://cymx66688.github.io/blog/archives/test555/</link>
      <guid>https://cymx66688.github.io/blog/archives/test555/</guid>
      <pubDate>Thu, 14 May 2020 10:39:54 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;将6605条预测数据划分为&lt;code&gt;优生&lt;/code&gt;和&lt;code&gt;非优生&lt;/code&gt;，过滤出&lt;code&gt;非优生&lt;/code&gt;共5229条。采用IsolationForest算法。异常分值的计算公式为：&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=
        
      
      </description>
      
      
      <content:encoded><![CDATA[<p>将6605条预测数据划分为<code>优生</code>和<code>非优生</code>，过滤出<code>非优生</code>共5229条。采用IsolationForest算法。异常分值的计算公式为：</p><script type="math/tex; mode=display">s(x, n) = 2^{-\frac{E(h(x))}{c(n)}}</script><p>其中：<br>$c(n) = 2H(n-1)-(2(n-1)/n)$<br>$H(n-1)=ln(n-1)+0.5772156649$  </p><p>$E(h(x))$为树深度的平均值</p><p>当$E(h(x))→c(n)，s→0.5$;<br>当$E(h(x))→0，s→1$;<br>当$E(h(x))→n-1，s→0$; </p>]]></content:encoded>
      
      <comments>https://cymx66688.github.io/blog/archives/test555/#disqus_thread</comments>
    </item>
    
    <item>
      <title>test444</title>
      <link>https://cymx66688.github.io/blog/archives/test444/</link>
      <guid>https://cymx66688.github.io/blog/archives/test444/</guid>
      <pubDate>Thu, 14 May 2020 09:10:49 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;本文作者： Kelley&lt;br&gt;本文链接： &lt;a href=&quot;https://cymx66688.github.io/blog/2020/05/14/test444/&quot;&gt;https://cymx66688.github.io/blog/2020/05/14/test444/
        
      
      </description>
      
      
      <content:encoded><![CDATA[<p>本文作者： Kelley<br>本文链接： <a href="https://cymx66688.github.io/blog/2020/05/14/test444/">https://cymx66688.github.io/blog/2020/05/14/test444/</a><br>版权声明： 本博客所有文章除特别声明外，均采用 CC BY-NC-SA 3.0 许可协议。转载请注明出处！</p>]]></content:encoded>
      
      <comments>https://cymx66688.github.io/blog/archives/test444/#disqus_thread</comments>
    </item>
    
  </channel>
</rss>
